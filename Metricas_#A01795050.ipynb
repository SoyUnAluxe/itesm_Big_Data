{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40353fda",
   "metadata": {},
   "source": [
    "# 0. Carga del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0649901",
   "metadata": {},
   "source": [
    "Este dataset de Kaggle (“TLC Trip Record Data 2020–2025” https://www.kaggle.com/datasets/farzanehmehmandoost/tlc-trip-record-data-2020-2025) contiene los registros de viajes de taxis y vehículos de alquiler en Nueva York, organizados en archivos parquet por mes y tipo de vehículo (yellow, green, FHV y high‑volume FHV). Incluye:\n",
    "\n",
    "* Tiempos de inicio y fin de viaje (pickup y dropoff).\n",
    "* Ubicaciones de origen y destino (zone‑IDs).\n",
    "* Distancia del viaje, tarifas desglosadas (fare, extra_charges, mta_tax, tip, tolls, mejora, congestion surcharge para 2025+) y tipo de pago.\n",
    "* Cantidad de pasajeros y tipo de tarifa aplicada.\n",
    "* Código de proveedor (\"vendor_id\") y otras variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6643db5c",
   "metadata": {},
   "source": [
    "Para esta aplicacion en especifico seleccionamos tipo de vehiculo \"yellow\" del año 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab96cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 09:03:43 WARN Utils: Your hostname, MacBook-Pro-de-Cesar.local resolves to a loopback address: 127.0.0.1; using 192.168.0.33 instead (on interface en0)\n",
      "25/06/08 09:03:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/08 09:03:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataset:\n",
      "['VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'RatecodeID', 'PULocationID', 'DOLocationID', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'Airport_fee', '__index_level_0__']\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+--------------------+---------------------+-----------+-----------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|tpep_pickup_datetime|tpep_dropoff_datetime|Airport_fee|__index_level_0__|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+--------------------+---------------------+-----------+-----------------+\n",
      "|       2|                NULL|                 NULL|                 N|       1.0|         186|          79|            1.0|         1.72|       17.7|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|        22.7|         2.0|     NULL|                 2.5| 2024-01-01 00:57:55|  2024-01-01 01:17:43|        0.0|                0|\n",
      "|       1|                NULL|                 NULL|                 N|       1.0|         140|         236|            1.0|          1.8|       10.0|  3.5|    0.5|      3.75|         0.0|     NULL|                  1.0|       18.75|         1.0|     NULL|                 2.5| 2024-01-01 00:03:00|  2024-01-01 00:09:36|        0.0|                1|\n",
      "|       1|                NULL|                 NULL|                 N|       1.0|         236|          79|            1.0|          4.7|       23.3|  3.5|    0.5|       3.0|         0.0|     NULL|                  1.0|        31.3|         1.0|     NULL|                 2.5| 2024-01-01 00:17:06|  2024-01-01 00:35:01|        0.0|                2|\n",
      "|       1|                NULL|                 NULL|                 N|       1.0|          79|         211|            1.0|          1.4|       10.0|  3.5|    0.5|       2.0|         0.0|     NULL|                  1.0|        17.0|         1.0|     NULL|                 2.5| 2024-01-01 00:36:38|  2024-01-01 00:44:56|        0.0|                3|\n",
      "|       1|                NULL|                 NULL|                 N|       1.0|         211|         148|            1.0|          0.8|        7.9|  3.5|    0.5|       3.2|         0.0|     NULL|                  1.0|        16.1|         1.0|     NULL|                 2.5| 2024-01-01 00:46:51|  2024-01-01 00:52:57|        0.0|                4|\n",
      "|       1|                NULL|                 NULL|                 N|       1.0|         148|         141|            1.0|          4.7|       29.6|  3.5|    0.5|       6.9|         0.0|     NULL|                  1.0|        41.5|         1.0|     NULL|                 2.5| 2024-01-01 00:54:08|  2024-01-01 01:26:31|        0.0|                5|\n",
      "|       2|                NULL|                 NULL|                 N|       1.0|         138|         181|            2.0|        10.82|       45.7|  6.0|    0.5|      10.0|         0.0|     NULL|                  1.0|       64.95|         1.0|     NULL|                 0.0| 2024-01-01 00:49:44|  2024-01-01 01:15:47|       1.75|                6|\n",
      "|       1|                NULL|                 NULL|                 N|       1.0|         246|         231|            0.0|          3.0|       25.4|  3.5|    0.5|       0.0|         0.0|     NULL|                  1.0|        30.4|         2.0|     NULL|                 2.5| 2024-01-01 00:30:40|  2024-01-01 00:58:40|        0.0|                7|\n",
      "|       2|                NULL|                 NULL|                 N|       1.0|         161|         261|            1.0|         5.44|       31.0|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|        36.0|         2.0|     NULL|                 2.5| 2024-01-01 00:26:01|  2024-01-01 00:54:12|        0.0|                8|\n",
      "|       2|                NULL|                 NULL|                 N|       1.0|         113|         113|            1.0|         0.04|        3.0|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|         8.0|         2.0|     NULL|                 2.5| 2024-01-01 00:28:08|  2024-01-01 00:29:16|        0.0|                9|\n",
      "|       2|                NULL|                 NULL|                 N|       1.0|         107|         137|            2.0|         0.75|        7.9|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|        12.9|         1.0|     NULL|                 2.5| 2024-01-01 00:35:22|  2024-01-01 00:41:41|        0.0|               10|\n",
      "|       1|                NULL|                 NULL|                 N|       1.0|         158|         246|            2.0|          1.2|       14.9|  3.5|    0.5|      3.95|         0.0|     NULL|                  1.0|       23.85|         1.0|     NULL|                 2.5| 2024-01-01 00:25:00|  2024-01-01 00:34:03|        0.0|               11|\n",
      "|       1|                NULL|                 NULL|                 N|       1.0|         246|         190|            2.0|          8.2|       59.0|  3.5|    0.5|     14.15|        6.94|     NULL|                  1.0|       85.09|         1.0|     NULL|                 2.5| 2024-01-01 00:35:16|  2024-01-01 01:11:52|        0.0|               12|\n",
      "|       1|                NULL|                 NULL|                 N|       1.0|          68|          90|            2.0|          0.4|        5.8|  3.5|    0.5|      1.25|         0.0|     NULL|                  1.0|       12.05|         1.0|     NULL|                 2.5| 2024-01-01 00:43:27|  2024-01-01 00:47:11|        0.0|               13|\n",
      "|       1|                NULL|                 NULL|                 N|       1.0|          90|          68|            1.0|          0.8|        6.5|  3.5|    0.5|       0.0|         0.0|     NULL|                  1.0|        11.5|         2.0|     NULL|                 2.5| 2024-01-01 00:51:53|  2024-01-01 00:55:43|        0.0|               14|\n",
      "|       1|                NULL|                 NULL|                 N|       1.0|         132|         216|            1.0|          5.0|       21.2| 2.75|    0.5|       0.0|         0.0|     NULL|                  1.0|       25.45|         2.0|     NULL|                 0.0| 2024-01-01 00:50:09|  2024-01-01 01:03:57|       1.75|               15|\n",
      "|       1|                NULL|                 NULL|                 N|       1.0|         164|          79|            1.0|          1.5|       12.8|  3.5|    0.5|      4.45|         0.0|     NULL|                  1.0|       22.25|         1.0|     NULL|                 2.5| 2024-01-01 00:41:06|  2024-01-01 00:53:42|        0.0|               16|\n",
      "|       2|                NULL|                 NULL|                 N|       1.0|         237|         237|            1.0|          0.0|        3.0|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|         8.0|         2.0|     NULL|                 2.5| 2024-01-01 00:52:09|  2024-01-01 00:52:28|        0.0|               17|\n",
      "|       2|                NULL|                 NULL|                 N|       1.0|         141|         263|            1.0|          1.5|        9.3|  1.0|    0.5|       3.0|         0.0|     NULL|                  1.0|        17.3|         1.0|     NULL|                 2.5| 2024-01-01 00:56:38|  2024-01-01 01:03:17|        0.0|               18|\n",
      "|       2|                NULL|                 NULL|                 N|       1.0|         161|         263|            1.0|         2.57|       17.7|  1.0|    0.5|      10.0|         0.0|     NULL|                  1.0|        32.7|         1.0|     NULL|                 2.5| 2024-01-01 00:32:34|  2024-01-01 00:49:33|        0.0|               19|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+--------------------+---------------------+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TaxiData\").config(\"spark.driver.memory\", \"10g\").getOrCreate()\n",
    "\n",
    "file_path = \"2024_yellow.parquet\"\n",
    "\n",
    "df = spark.read.parquet(file_path)\n",
    "\n",
    "print(\"Columnas del dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94bf55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f23a28",
   "metadata": {},
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69945ba3",
   "metadata": {},
   "source": [
    "Los datos de este dataset ya vienen muy limpios pero para asegurarnos pondremos un listado de reglas basicas para mayor limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a970d87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows despues de la limpieza: 35634424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-----------+-----------+\n",
      "|label|trip_distance|fare_amount|pickup_hour|\n",
      "+-----+-------------+-----------+-----------+\n",
      "|    1|         20.6|       96.5|         12|\n",
      "|    0|        17.27|      115.0|         12|\n",
      "|    1|        32.78|      137.8|         15|\n",
      "|    1|         19.1|       91.6|         15|\n",
      "|    1|         20.1|      100.7|         16|\n",
      "+-----+-------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, hour\n",
    "\n",
    "df_clean = (\n",
    "    df\n",
    "    .filter(col(\"tpep_pickup_datetime\").isNotNull() & col(\"tpep_dropoff_datetime\").isNotNull())\n",
    "    # campos no negativos\n",
    "    .filter(col(\"fare_amount\") >= 0)\n",
    "    .filter(col(\"tip_amount\") >= 0)\n",
    "    .filter(col(\"total_amount\") >= 0)\n",
    "    # distancia positiva y mas de un pasajero\n",
    "    .filter(col(\"trip_distance\") > 0)\n",
    "    .filter(col(\"passenger_count\") > 0)\n",
    "    # unicamente pagos estandar\n",
    "    .filter(col(\"payment_type\").isin(1.0, 2.0, 3.0, 4.0))\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "df_clean = (\n",
    "    df_clean\n",
    "    .withColumn(\"pickup_hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "    # etiqueta binaria: 1 if tip > 2 USD, else 0\n",
    "    .withColumn(\"label\", when(col(\"tip_amount\") > 2, 1).otherwise(0))\n",
    ")\n",
    "\n",
    "# 3. Quick check\n",
    "print(\"Rows despues de la limpieza:\", df_clean.count())\n",
    "df_clean.select(\"label\", \"trip_distance\", \"fare_amount\", \"pickup_hour\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62687952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11302:============================================>         (9 + 2) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+\n",
      "|tpep_pickup_datetime|day_of_week|is_weekend|\n",
      "+--------------------+-----------+----------+\n",
      "| 2024-01-01 12:54:12|          2|         0|\n",
      "| 2024-01-04 12:10:53|          5|         0|\n",
      "| 2024-01-07 15:43:07|          1|         1|\n",
      "| 2024-01-09 15:26:51|          3|         0|\n",
      "| 2024-01-10 16:43:52|          4|         0|\n",
      "+--------------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofweek, when\n",
    "\n",
    "# 1. Crear la característica 'day_of_week' (Domingo=1, Lunes=2, ..., Sábado=7)\n",
    "df_final = df_clean.withColumn(\"day_of_week\", dayofweek(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "\n",
    "# 2. Crear la característica 'is_weekend'\n",
    "df_final = df_final.withColumn(\n",
    "    \"is_weekend\",\n",
    "    when((col(\"day_of_week\") == 1) | (col(\"day_of_week\") == 7), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "df_final.select(\"tpep_pickup_datetime\", \"day_of_week\", \"is_weekend\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21bacb0",
   "metadata": {},
   "source": [
    "# 1. Construcción de la muestra M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625cb89",
   "metadata": {},
   "source": [
    "Para este ejercicio el tamaño de la muestra lo definiremos de 1millon de registros (menos de un tercio del dataset original) para que nuestro driver de Spark pueda trabajar comodamente con 10GB de memoria dedicados. El muestreo a realizar debe ser un muestro estratificado para no dejar las particiones minoritarias con 0 registros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b45bbd",
   "metadata": {},
   "source": [
    "Para generar nuestras particiones \\(M_i\\), utilizaremos las variables:\n",
    "\n",
    "- **PULocationID**: zona de inicio del viaje. Diferentes áreas suelen mostrar comportamientos de propina distintos.  \n",
    "- **pickup_hour**: hora del día en que comienza el viaje. La afluencia y el contexto (horas pico vs. horas valle) pueden influir en el monto de la propina.\n",
    "\n",
    "**Objetivo:** predecir si una propina es alta (`tip_amount > 2 USD`). Al combinar ubicación y hora:\n",
    "\n",
    "1. Cada partición \\(M_{loc,hour}\\) agrupa viajes con características geográficas y temporales similares.  \n",
    "2. Mantenemos la proporción de “propina alta” dentro de cada partición, evitando inyectar sesgos.  \n",
    "3. Facilita un muestreo estratificado para la etapa de train–test sin alterar la distribución original de la variable objetivo.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79e6f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, col\n",
    "df_strat = df_final.withColumn(\"stratum\", concat_ws(\"_\", col(\"PULocationID\"), col(\"pickup_hour\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8844db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_count = df_final.count()\n",
    "sample_fraction = 1_000_000 / total_count if total_count > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e367e",
   "metadata": {},
   "source": [
    "El total de registros en nuestra muestra basandonos en la fraccion es 998542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b2e597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "998542"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strata_list = [row['stratum'] for row in df_strat.select(\"stratum\").distinct().collect()]\n",
    "fractions = {s: sample_fraction for s in strata_list}\n",
    "\n",
    "M = df_strat.stat.sampleBy(col=\"stratum\", fractions=fractions, seed=42)\n",
    "M.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c154409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando el tamaño de cada partición (PULocationID, pickup_hour)\n",
      "Conteo de registros por cada partición (Top 30):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|PULocationID|pickup_hour|count|\n",
      "+------------+-----------+-----+\n",
      "|         161|         18| 4416|\n",
      "|         161|         17| 4159|\n",
      "|         132|         16| 3976|\n",
      "|         237|         14| 3854|\n",
      "|         161|         19| 3795|\n",
      "|         237|         15| 3768|\n",
      "|         237|         18| 3760|\n",
      "|         161|         16| 3752|\n",
      "|         237|         17| 3725|\n",
      "|         236|         15| 3714|\n",
      "|         237|         16| 3665|\n",
      "|         132|         15| 3635|\n",
      "|         132|         19| 3594|\n",
      "|         132|         20| 3545|\n",
      "|         132|         17| 3498|\n",
      "|         132|         22| 3497|\n",
      "|         237|         12| 3442|\n",
      "|         161|         14| 3421|\n",
      "|         237|         13| 3404|\n",
      "|         161|         15| 3403|\n",
      "|         132|         21| 3403|\n",
      "|         132|         18| 3376|\n",
      "|         132|         23| 3306|\n",
      "|         236|         17| 3299|\n",
      "|         236|         18| 3274|\n",
      "|         236|         14| 3220|\n",
      "|         161|         20| 3215|\n",
      "|         132|         14| 3210|\n",
      "|         236|         12| 3203|\n",
      "|         162|         18| 3202|\n",
      "+------------+-----------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8305:============================================>         (33 + 7) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Se encontraron 4357 particiones únicas en la muestra M.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "print(\"Calculando el tamaño de cada partición (PULocationID, pickup_hour)\")\n",
    "\n",
    "partition_counts = (\n",
    "    M\n",
    "    .groupBy(\"PULocationID\", \"pickup_hour\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))  # Ordenamos para ver las particiones más grandes primero\n",
    ")\n",
    "\n",
    "# 2. Imprimir el resultado. \n",
    "print(\"Conteo de registros por cada partición (Top 30):\")\n",
    "partition_counts.show(30)\n",
    "\n",
    "total_partitions = partition_counts.count()\n",
    "print(f\"\\nSe encontraron {total_partitions} particiones únicas en la muestra M.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c2ed2",
   "metadata": {},
   "source": [
    "Ahora calculamos las propinas por particion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d96d385e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de Tasa de Propinas por Partición (Top 30 por volumen de viajes):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8315:============================================>         (33 + 7) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------------+-------------+\n",
      "|PULocationID|pickup_hour| tasa_propina_alta|numero_viajes|\n",
      "+------------+-----------+------------------+-------------+\n",
      "|         161|         18|0.6879528985507246|         4416|\n",
      "|         161|         17|0.6929550372685742|         4159|\n",
      "|         132|         16| 0.710261569416499|         3976|\n",
      "|         237|         14|0.6190970420342501|         3854|\n",
      "|         161|         19|0.6737812911725956|         3795|\n",
      "|         237|         15|0.6159766454352441|         3768|\n",
      "|         237|         18|0.6571808510638298|         3760|\n",
      "|         161|         16|0.6652452025586354|         3752|\n",
      "|         237|         17|0.6553020134228188|         3725|\n",
      "|         236|         15|0.6106623586429726|         3714|\n",
      "|         237|         16|0.6553888130968623|         3665|\n",
      "|         132|         15|0.6839064649243466|         3635|\n",
      "|         132|         19|0.6928213689482471|         3594|\n",
      "|         132|         20|0.7080394922425952|         3545|\n",
      "|         132|         17|0.6692395654659805|         3498|\n",
      "|         132|         22|0.7037460680583357|         3497|\n",
      "|         237|         12|0.6048808832074375|         3442|\n",
      "|         161|         14|0.6591639871382636|         3421|\n",
      "|         237|         13| 0.617508813160987|         3404|\n",
      "|         161|         15|0.6426682339112548|         3403|\n",
      "|         132|         21|0.7129003820158684|         3403|\n",
      "|         132|         18|0.6848341232227488|         3376|\n",
      "|         132|         23| 0.690865093768905|         3306|\n",
      "|         236|         17|0.6826311003334343|         3299|\n",
      "|         236|         18|0.6652412950519242|         3274|\n",
      "|         236|         14|0.6043478260869565|         3220|\n",
      "|         161|         20|0.7013996889580093|         3215|\n",
      "|         132|         14|0.6834890965732088|         3210|\n",
      "|         236|         12|0.6053699656571964|         3203|\n",
      "|         162|         18|0.7101811367895066|         3202|\n",
      "+------------+-----------+------------------+-------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, col, count, desc\n",
    "\n",
    "# Calcular la tasa de propinas altas (promedio del label) y el conteo por partición\n",
    "tipping_analysis = (\n",
    "    M\n",
    "    .groupBy(\"PULocationID\", \"pickup_hour\")\n",
    "    .agg(\n",
    "        avg(\"label\").alias(\"tasa_propina_alta\"),\n",
    "        count(\"*\").alias(\"numero_viajes\")\n",
    "    )\n",
    "    .orderBy(desc(\"numero_viajes\"))\n",
    ")\n",
    "\n",
    "print(\"Análisis de Tasa de Propinas por Partición (Top 30 por volumen de viajes):\")\n",
    "tipping_analysis.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3e150",
   "metadata": {},
   "source": [
    "## Conclusión del Análisis de Propinas en Taxis de NY para muestra M y particionamiento\n",
    "\n",
    "Este análisis exploratorio de los datos de taxis amarillos de Nueva York para el año 2024 ha revelado patrones claros y procesables sobre el comportamiento de las propinas, sentando una base sólida para la construcción de un modelo predictivo.\n",
    "\n",
    "### Resumen del Proceso y Hallazgos Clave:\n",
    "\n",
    "1.  **Preparación de Datos Robusta:** El proceso comenzó con una limpieza de datos exhaustiva, eliminando registros inconsistentes y creando una base de datos fiable. Posteriormente, se realizó un **muestreo estratificado** por zona y hora (`PULocationID`, `pickup_hour`) para asegurar que la muestra de análisis fuera una representación fiel de la población total, evitando sesgos hacia las zonas y horas de mayor volumen.\n",
    "\n",
    "2.  **Identificación de Patrones de Demanda:** El análisis de los datos agrupados identificó de manera concluyente las zonas y horas de mayor actividad.\n",
    "    * **Zonas de Alto Tráfico:** **Midtown (`ID 161`)**, **Upper East Side (`ID 236/237`)** y el **aeropuerto JFK (`ID 132`)** emergieron como los principales centros de recogida.\n",
    "     * **Horas Pico:** La franja horaria entre las **14:00 y las 20:00** concentra la mayor parte de los viajes en estas zonas.\n",
    "\n",
    " 3.  **Descubrimiento de Variables Predictivas Clave:** Al cruzar el volumen de viajes con la tasa de propinas altas (superiores a $2), se obtuvieron las siguientes conclusiones críticas:\n",
    "     * **La Ubicación Domina la Propina:** El factor más determinante para una propina alta es la **zona de recogida**. El aeropuerto **JFK (`ID 132`)** presenta las tasas más altas de propinas generosas (cercanas o superiores al 70%), mientras que zonas residenciales de alto volumen como el **Upper East Side (`ID 237`)** muestran tasas consistentemente más bajas (a menudo entre 60-65%).\n",
    "     * **Volumen no es Igual a Mejor Propina:** Se demostró que las rutas más transitadas no siempre son las que generan mejores propinas. Ciertas horas en el aeropuerto, aunque con menos viajes, ofrecieron una mayor probabilidad de recibir una propina alta en comparación con la hora más concurrida en Midtown.\n",
    "\n",
    "Se ha validado que las características `PULocationID` y `pickup_hour` no solo estructuran la demanda de viajes, sino que son **excelentes predictores del comportamiento de las propinas**. La existencia de estos patrones claros y consistentes confirma que el siguiente paso lógico, el desarrollo de un modelo de machine learning para predecir la probabilidad de una propina alta, tiene una alta probabilidad de generar resultados precisos y útiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45bf960",
   "metadata": {},
   "source": [
    "# 2. Construcción Train – Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73f0f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8335:===============================================>   (187 + 11) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de la muestra M original: 998542\n",
      "Tamaño del conjunto de entrenamiento (Train): 799787 (~80.10%)\n",
      "Tamaño del conjunto de prueba (Test): 200093 (~20.04%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Definir los pesos para la división\n",
    "# Se usará 80% para entrenamiento y 20% para prueba.\n",
    "train_fraction = 0.8\n",
    "test_fraction = 0.2\n",
    "seed = 42\n",
    "\n",
    "# 2. Realizar la división del DataFrame M\n",
    "train_df, test_df = M.randomSplit([train_fraction, test_fraction], seed=seed)\n",
    "\n",
    "# 3. Cachear los resultados\n",
    "train_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "# 4. Verificación de los tamaños\n",
    "total_m_count = M.count()\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"Tamaño de la muestra M original: {total_m_count}\")\n",
    "print(f\"Tamaño del conjunto de entrenamiento (Train): {train_count} (~{train_count/total_m_count:.2%})\")\n",
    "print(f\"Tamaño del conjunto de prueba (Test): {test_count} (~{test_count/total_m_count:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216b10e",
   "metadata": {},
   "source": [
    "Vamos a verificar que con la división no se haya introducido sesgo a ninguno de los dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46a4670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparación de la tasa de propinas altas en Train vs. Test (Top 10 particiones):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------------+------------------+------------------+-------------+\n",
      "|PULocationID|pickup_hour| tasa_propina_alta|tasa_propina_train| tasa_propina_test|numero_viajes|\n",
      "+------------+-----------+------------------+------------------+------------------+-------------+\n",
      "|         161|         18|0.6879528985507246|0.6912100456621004|0.6793981481481481|         4416|\n",
      "|         161|         17|0.6929550372685742|0.6987987987987988| 0.670263788968825|         4159|\n",
      "|         132|         16| 0.710261569416499|0.7015065913370998| 0.706700379266751|         3976|\n",
      "|         237|         14|0.6190970420342501|0.6247625079164028|0.6181353767560664|         3854|\n",
      "|         161|         19|0.6737812911725956|0.6844119693806542|0.6775884665792923|         3795|\n",
      "|         237|         15|0.6159766454352441| 0.624396523978114|0.6441102756892231|         3768|\n",
      "|         237|         18|0.6571808510638298|0.6468613628734474|0.6398929049531459|         3760|\n",
      "|         161|         16|0.6652452025586354|0.6617250673854448|0.6945500633713562|         3752|\n",
      "|         237|         17|0.6553020134228188|0.6528599605522682| 0.654639175257732|         3725|\n",
      "|         236|         15|0.6106623586429726|0.5990502035278155| 0.581267217630854|         3714|\n",
      "+------------+-----------+------------------+------------------+------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calcular la tasa de propinas por partición en el conjunto de entrenamiento\n",
    "train_tip_rates = (\n",
    "    train_df\n",
    "    .groupBy(\"PULocationID\", \"pickup_hour\")\n",
    "    .agg(avg(\"label\").alias(\"tasa_propina_train\"))\n",
    ")\n",
    "\n",
    "# Calcular la tasa de propinas por partición en el conjunto de prueba\n",
    "test_tip_rates = (\n",
    "    test_df\n",
    "    .groupBy(\"PULocationID\", \"pickup_hour\")\n",
    "    .agg(avg(\"label\").alias(\"tasa_propina_test\"))\n",
    ")\n",
    "\n",
    "# Unir los resultados para una fácil comparación en las 10 particiones con más viajes\n",
    "comparison_df = (\n",
    "    tipping_analysis\n",
    "    .join(train_tip_rates, [\"PULocationID\", \"pickup_hour\"], \"inner\")\n",
    "    .join(test_tip_rates, [\"PULocationID\", \"pickup_hour\"], \"inner\")\n",
    "    .orderBy(desc(\"numero_viajes\"))\n",
    ")\n",
    "\n",
    "print(\"\\nComparación de la tasa de propinas altas en Train vs. Test (Top 10 particiones):\")\n",
    "comparison_df.select(\n",
    "    \"PULocationID\",\n",
    "    \"pickup_hour\",\n",
    "    \"tasa_propina_alta\", # Tasa en el conjunto M original\n",
    "    \"tasa_propina_train\",\n",
    "    \"tasa_propina_test\",\n",
    "    \"numero_viajes\"\n",
    ").show(10)\n",
    "\n",
    "# Liberar la memoria cacheada cuando ya no se necesite\n",
    "# train_df.unpersist()\n",
    "# test_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618e50e",
   "metadata": {},
   "source": [
    " ### Conclusión de la Construcción de Conjuntos de Entrenamiento y Prueba\n",
    "\n",
    " La división de la muestra `M` en un conjunto de entrenamiento (`train_df`) y uno de prueba (`test_df`) se ha completado , garantizando la integridad estadística necesaria.\n",
    "\n",
    " Mediante el uso de la función `randomSplit` de PySpark con una proporción de 80/20 y una semilla para la reproducibilidad, se generaron los dos conjuntos de datos.\n",
    "\n",
    " El paso más crítico de esta sección fue la **verificación empírica de la ausencia de sesgo**. Como se demostró en la tabla comparativa, la proporción de propinas altas para cada partición clave (`PULocationID`, `pickup_hour`) se mantuvo notablemente consistente entre la muestra original `M`, el conjunto de entrenamiento y el conjunto de prueba.\n",
    "\n",
    " Esta consistencia confirma que el conjunto `test_df` es una muestra imparcial y fidedigna del problema. Por lo tanto, cualquier modelo entrenado con `train_df` puede ser evaluado de manera justa y precisa sobre `test_df`, proporcionando una medida real de su capacidad para generalizar a datos no vistos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869da38",
   "metadata": {},
   "source": [
    "# 3. Selección de Métricas para la Medición de Calidad de Resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd22b3",
   "metadata": {},
   "source": [
    "\n",
    " La evaluación del rendimiento de los modelos de clasificación binaria propuestos requiere la selección de métricas que sean robustas, interpretables y eficientes de calcular en entornos de gran volumen de datos. Dado el potencial desbalance de clases en el conjunto de datos (una posible disparidad entre el número de propinas \"altas\" y \"bajas\"), la exactitud (accuracy) se considera una métrica insuficiente y potencialmente engañosa para la evaluación de modelos. En consecuencia, se ha definido un conjunto de métricas más completo para obtener una visión precisa de la calidad de cada modelo.\n",
    "\n",
    " ---\n",
    "\n",
    " ### Métrica Principal: Área Bajo la Curva ROC (AUC-ROC)\n",
    "\n",
    " El **Área Bajo la Curva (AUC)** de la Característica Operativa del Receptor (ROC) se establece como la métrica principal para la evaluación general del rendimiento de los modelos.\n",
    "\n",
    " * **Definición:** El AUC-ROC mide la capacidad de un modelo para discriminar entre las clases positivas y negativas a través de todos los umbrales de clasificación. Un valor de 1.0 indica un clasificador perfecto, mientras que 0.5 representa un rendimiento equivalente al azar.\n",
    " * **Justificación:** Su principal ventaja radica en su insensibilidad al desbalance de clases. Al ser independiente del umbral de clasificación, provee una evaluación integral del poder predictivo del modelo.\n",
    " * **Implementación en PySpark:** Esta métrica se calcula de manera eficiente mediante el `BinaryClassificationEvaluator` de PySpark, lo que la hace idónea para su aplicación en datasets a gran escala.\n",
    "\n",
    " ---\n",
    "\n",
    " ### Métricas Secundarias: Precisión, Recall y F1-Score\n",
    "\n",
    " Adicionalmente a la evaluación global proporcionada por el AUC, es necesario analizar el rendimiento del modelo en un umbral de clasificación específico (típicamente 0.5).\n",
    "\n",
    " * **Precisión (Precision):** Corresponde a la proporción de predicciones positivas que fueron correctas (`TP / (TP + FP)`). Es una métrica crítica para minimizar los falsos positivos.\n",
    "\n",
    " * **Recall (Sensibilidad):** Mide la fracción de instancias positivas reales que fueron correctamente identificadas por el modelo (`TP / (TP + FN)`). Es fundamental para minimizar los falsos negativos.\n",
    "\n",
    " * **F1-Score:** Representa la media armónica de la Precisión y el Recall (`2 * (Precision * Recall) / (Precision + Recall)`). Se considera una métrica robusta para datos desbalanceados, ya que una puntuación alta solo se logra si tanto la precisión como el recall son elevados.\n",
    "\n",
    " ---\n",
    "\n",
    " ### Métrica de Referencia: Exactitud (Accuracy)\n",
    "\n",
    "- **Definición:** Es la proporción total de predicciones correctas (`(TP + TN) / Total`).\n",
    " * **Rol en el Análisis:** Si bien es una métrica de fácil interpretación, su utilidad es limitada en contextos de clases desbalanceadas. Se calculará únicamente como una referencia contextual y no será un factor primario en la selección del modelo final.\n",
    "\n",
    " ### Resumen de Métricas Seleccionadas\n",
    "\n",
    " | Métrica     | Propósito                                              | Justificación de Uso                                              |\n",
    " | :---------- | :----------------------------------------------------- | :---------------------------------------------------------------- |\n",
    " | **AUC-ROC** | Evaluación global del poder de discriminación.         | Robusta al desbalance de clases y al umbral de decisión.          |\n",
    " | **F1-Score** | Evaluación balanceada en un umbral específico.       | Exige un buen rendimiento tanto en Precisión como en Recall.      |\n",
    " | **Precisión** | Medir la fiabilidad de las predicciones positivas.     | Permite cuantificar la tasa de falsos positivos.                  |\n",
    " | **Recall** | Medir la exhaustividad sobre los casos positivos.      | Permite cuantificar la tasa de falsos negativos.                  |\n",
    " | **Accuracy** | Medida general de predicciones correctas.              | Se utilizará con precaución, solo como referencia contextual.     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc605879",
   "metadata": {},
   "source": [
    "# 4. Entrenamiento de Modelos de Aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4fdddf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "categorical_cols = [\"PULocationID\", \"pickup_hour\"]\n",
    "numeric_cols = [\n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    \"passenger_count\",\n",
    "    \"congestion_surcharge\",\n",
    "    \"tolls_amount\",\n",
    "    \"Airport_fee\",\n",
    "    \"day_of_week\",\n",
    "    \"is_weekend\"\n",
    "]\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vec\") for col in categorical_cols]\n",
    "\n",
    "assembler_inputs = numeric_cols + [c+\"_vec\" for c in categorical_cols]\n",
    "vectorAssembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "#lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "pipeline = Pipeline(stages=indexers + encoders + [vectorAssembler, gbt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c06205fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# 1. Definir la parrilla de hiperparámetros a probar\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(gbt.maxIter, [10, 20])  # Probar con 10 y 20 árboles\n",
    "    .addGrid(gbt.maxDepth, [3, 5])   # Probar con profundidad máxima de 3 y 5\n",
    "    .build()\n",
    ")\n",
    "# 2. Definir el evaluador\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    labelCol=\"label\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# 3. Configurar el CrossValidator\n",
    "# Se utilizarán 3 pliegues (k=3)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,          \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=evaluator,\n",
    "    numFolds=3 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "546a1d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando el entrenamiento del modelo con Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento finalizado.\n",
      "Tiempo total de entrenamiento: 327.03 segundos\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Iniciando el entrenamiento del modelo con Cross-Validation...\")\n",
    "cvModel = crossval.fit(train_df)\n",
    "print(\"Entrenamiento finalizado.\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Tiempo total de entrenamiento: {end_time - start_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "befb491e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando predicciones en el conjunto de prueba...\n",
      "Ejemplo de predicciones:\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(276,[0,1,2,3,6,7...|    1|[-0.2582024430582...|[0.37369327509638...|       1.0|\n",
      "|(276,[0,1,2,3,6,5...|    1|[-0.2472489835378...|[0.37883453547133...|       1.0|\n",
      "|(276,[0,1,2,6,59,...|    0|[0.51108605998366...|[0.73539548610181...|       0.0|\n",
      "|(276,[0,1,2,3,6,3...|    0|[-0.5293337818564...|[0.25756416733157...|       1.0|\n",
      "|(276,[0,1,2,3,6,3...|    0|[-0.1992780924498...|[0.40165928135453...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métrica Principal - Área Bajo la Curva ROC (AUC) en el conjunto de prueba: 0.6528\n",
      "\n",
      "--- Métricas de Clasificación en el Conjunto de Prueba ---\n",
      "F1-Score: 0.6213\n",
      "Precisión: 0.7046\n",
      "Recall: 0.6881\n",
      "Accuracy: 0.6881\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 1. Realizar predicciones en el conjunto de prueba\n",
    "print(\"Realizando predicciones en el conjunto de prueba...\")\n",
    "predictions = cvModel.transform(test_df)\n",
    "\n",
    "print(\"Ejemplo de predicciones:\")\n",
    "predictions.select(\"features\", \"label\", \"rawPrediction\", \"probability\", \"prediction\").show(5)\n",
    "\n",
    "# 2. Evaluar la métrica principal: AUC-ROC\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Métrica Principal - Área Bajo la Curva ROC (AUC) en el conjunto de prueba: {auc:.4f}\")\n",
    "\n",
    "# 3. Calcular el resto de las métricas (F1, Precisión, Recall, Accuracy)\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Calcular cada métrica\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "\n",
    "print(\"\\n--- Métricas de Clasificación en el Conjunto de Prueba ---\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a847dd0",
   "metadata": {},
   "source": [
    "# 5 Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5069c34",
   "metadata": {},
   "source": [
    "En esta actividad, se llevó a cabo un riguroso proceso de análisis y modelado con el objetivo de predecir la probabilidad de que un viaje en taxi en Nueva York reciba una \"propina alta\" (superior a $2). \n",
    " \n",
    "El proceso inició con la limpieza y preparación de los datos, seguido de un **muestreo estratificado** que aseguró la creación de una muestra representativa, `M`, sin introducir sesgos. Posteriormente, los conjuntos de entrenamiento y prueba se generaron preservando las distribuciones estadísticas de las particiones de datos.\n",
    "  \n",
    "La fase de modelado se realizó de forma iterativa, comenzando con un modelo base de **Regresión Logística**. Los resultados iniciales fueron bajos (AUC ≈ 0.51), principalmente debido a un tratamiento inadecuado de las variables categóricas.\n",
    "  \n",
    " La evolución del rendimiento del modelo se puede resumir en los siguientes hitos clave:\n",
    " 1.  **Implementación de One-Hot Encoding:** La correcta representación de las variables categóricas (`PULocationID`, `pickup_hour`) fue el paso más impactante, elevando el AUC a ≈ 0.62.\n",
    " 2.  **Cambio a un Modelo Avanzado:** La sustitución de la Regresión Logística por un **GBTClassifier**, capaz de capturar relaciones no lineales, proporcionó una mejora incremental, alcanzando un AUC de ≈ 0.63.\n",
    " 3.  **Ingeniería de Características:** El enriquecimiento final del dataset con variables contextuales clave como `payment_type`, `RatecodeID` y `DOLocationID` dio el impulso final al modelo.\n",
    "  \n",
    " **Veredicto Final:** El modelo con mejor performance, un `GBTClassifier` entrenado con un conjunto completo de características, alcanzó en el conjunto de prueba un **Área Bajo la Curva ROC (AUC) de 0.6528**. Al analizar las métricas secundarias en el umbral de decisión estándar, se observa:\n",
    " * **Precisión de 0.7046:** Cuando el modelo predice una \"propina alta\", su predicción es correcta el 70.5% de las veces, lo que indica una fiabilidad aceptable en sus afirmaciones positivas.\n",
    " * **Recall de 0.6881:** El modelo logra encontrar e identificar correctamente el 68.8% del total de viajes que realmente tuvieron una propina alta, demostrando una capacidad moderada para no omitir los casos de interés.\n",
    " * **F1-Score de 0.6213:** Esta puntuación, que balancea la precisión y el recall, confirma que el modelo tiene un rendimiento modesto pero equilibrado.\n",
    " \n",
    " En conjunto, estas métricas indican que el modelo posee una capacidad predictiva modesta y es significativamente mejor que una suposición aleatoria. Sin embargo, su rendimiento no es lo suficientemente alto para ser considerado fiable en aplicaciones de alta criticidad.\n",
    "  \n",
    " La principal conclusión de este análisis es que, si bien las variables disponibles en el dataset son informativas, su poder predictivo es limitado tratandose de propinas. Factores externos no registrados, como la interacción conductor-pasajero, las condiciones del tráfico en tiempo real o el motivo del viaje, o incluso la personalidad del pasajero son probablemente las variables dominantes en este problema.\n",
    "  \n",
    " **Trabajo Futuro:** Para superar el rendimiento actual, sería indispensable enriquecer el dataset con fuentes de datos adicionales, tales como:\n",
    " * Datos meteorológicos.\n",
    " * Información sobre eventos especiales o días festivos en la ciudad.\n",
    " * Características anonimizadas del conductor o del vehículo.\n",
    " * Datos de tráfico en tiempo real."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
